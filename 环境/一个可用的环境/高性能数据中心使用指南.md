[toc]

运行作业的方式有两种：

一种是将计算过程写成脚本，通过`sbatch`指令提交到计算节点执行【不限时】；

另一种是通过`salloc`申请到计算节点，再ssh连接到计算节点进行计算【定时的】；

## modlue
```bash
# 显示帮助信息
$ module help 

# 查看可用软件环境
$ module avail
-------------------------- /public/software/modules/modulefiles ---------------------------
anaconda/anaconda3         compiler/cmake/3.20.1    modules                  
apps/abaqus/2020           compiler/gcc/7.3.1       mpi/intelmpi/2021.3.0    
apps/ansys/2021            compiler/intel/2021.3.0  mpi/openmpi/gnu/4.0.3    
apps/MaterialsStudio/2020  dot                      mpi/openmpi/intel/4.0.3  
apps/matlab/2021R          module-git               null                     
apps/vasp/intelmpi/5.4.4   module-info              use.own    




# load
$ module load anaconda/anaconda3 

# 列出已经导入的软件环境
$ module list   
Currently Loaded Modulefiles:
 1) anaconda/anaconda3 

# Unload all loaded modulefiles
$ module purge

# 删除相应的软件环境
$ module unload anaconda/anaconda3 

# 删除mod1并导入mod2
$ module switch [mod1] mod2 
```



## 容器

### 查看计算节点空闲状态
```bash
# drain(节点故障)，alloc(节点在用)，idle(节点可用)，down(节点下线)，mix(节点部分占用，但仍有剩余资源）
$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
A40          up 5-00:00:00      2    mix GPU[01-02]
GTX3090      up 5-00:00:00      7   idle GPU[03-09]
compute*     up 5-00:00:00      6   idle compute[01-06]
InpsurT4     up 5-00:00:00      2   idle tesla[01-02]
X620         up 7-00:00:00      4    mix GPU[10-13]
X620         up 7-00:00:00      4   idle GPU[14-17]
```

|p|N|n|gres|  |
| :------: | :------: | :--------------------: | :------: |:-: |
| 节点名称 | 节点数量 | 申请GPU资源时1块卡配n个核心数 | 几块GPU卡 | 内存 |
| A40      | 2 | 6 | 8 | 48GB |
| GTX3090  | 7 | 12 | 8 | 24GB |
| compute  | 6 | 12 | 用于纯CPU计算 | 4G |
| InspurT4 | 2 | 12 | 3 | 16GB  |
| X620     | 8 | 无 | 3 | 24GB|

### 申请

```bash
# salloc -p compute -N1 -n4 -t 1:00:00
-N: 几个节点nodes
-n: 几个核心

$ salloc -p A40 -N1 -n6 --gres=gpu:1 -t 0:30:00
salloc: Pending job allocation 4742
salloc: job 4742 queued and waiting for resources
salloc: job 4742 has been allocated resources
salloc: Granted job allocation 4742
salloc: Waiting for resource configuration
salloc: Nodes GPU01 are ready for job

$ ssh GPU01


```

#### 查看作业

```bash
$ ssh GPU01

# 查看作业是否还在运行，确保作业已经退出，避免产生不必要的费用
$ squeue -j 4742
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
              4742       A40 interact hpc71037  R       0:16      1 GPU01

# 计算资源使用完后取消作业，并退还未消费核时，不取消则运行至核时结束
$ scancel 4742   
```

#### 查看conda

```bash
$ ssh GPU01

[hpc2204081200015@GPU03 ~]$ module load anaconda/anaconda3
[hpc2204081200015@GPU03 ~]$ conda activate new
```



## jobs

### sbatch
```bash
-p, --partition=<partition_names> 
                    # -p,--partition compute          
                    # 作业提交的指定分区为compute；
                    # 查看分区：sacctmgr show ass user=`whoami` format=part |uniq
                    # compute, A40, GTX3090, GTX3090, InspurT4

-N, --nodes=N
                    # -N 1        
                    # 申请节点数为1,如果作业不能跨节点(MPI)运行, 申请的节点数应不超过1;
-n, --ntasks=ntasks         
                    # number of tasks to run
                    # 每个任务所需要的核心数，默认为1；
                    # ntasks 参数的优先级高于 ntasks-per-node ，如果使用--ntasks这个参数，那么将会变为每个节点最多运行的任务数；
--ntasks-per-node=<ntasks>   
                    # --ntasks-per-node=1 
                    # 每个任务所需要的核心数，默认为1；
                    # 每个节点上运行一个任务，默认情况下也可理解为每个节点使用一个核心，如果程序不支持多线程(如openmpi)，这个数不应该超过1；
--gres=<list>       # 使用gpu这类资源
                    # --gres=gpu:2
                    # 申请两块gpu



--help              # 显示帮助信息；


-o, --output=<filename pattern>
                    # -o job.%j.out       
                    # 脚本执行的输出将被保存在当job.%j.out文件下，%j表示作业号;
-J, --job-name=<jobname>    
                    # -J myFirstJob       
                    # 作业在调度系统中的作业名为myFirstJob;
-D, --chdir=<directory>      
                    # 指定工作目录；
-q, --qos=<qos>     # 指定QOS；
                    # --qos=low           
                    # 指定作业的QOS为low;
-t, --time=<time>   # 允许作业运行的最大时间，一般不需要指定运算完即退出。
                    # 格式为小时:分钟:秒数，如2:00:00（2小时），系统目前限定最大为120h(5天)；

-A <account>        # 指定计费账户；
--get-user-env      # 获取当前的环境变量；
--mail-type=<type>  # 指定状态发生时，发送邮件通知，有效种类为（NONE, BEGIN, END, FAIL, REQUEUE, ALL）；
--mail-user=<user>  # 发送给指定邮箱；
-c, --cpus-per-task=<ncpus>      
                    # 每个节点的任务数，--ntasks参数的优先级高于该参数，如果使用--ntasks这个参数，那么将会变为每个节点最多运行的任务数；
-w, --nodelist=<node name list>     
                    # 指定申请的节点；
-x, --exclude=<node name list>   
                    # 排除指定的节点；
```

### ssh

1. ssh

    ```bash
    ssh 你的上机账号@login01.hpc.nchu.edu.cn
    ```
2. `sbatch` 指令将作业提交到计算节点上执行

```BASH
#!/bin/bash
#SBATCH -J gnt
#SBATCH -o ~/fork/gnt.out
#SBATCH -p A40
#SBATCH -N 1 
#SBATCH -n 6
#SBATCH --gres=gpu:1  

module load anaconda/anaconda3 
source ~/envs/gnt/bin/activate
cd ~/fork/GNT
nvidia-smi
CUDA_VISIBLE_DEVICES=0 python train.py --config configs/gnt_blender.txt --i_img=1 --train_scenes lego --eval_scenes lego
```


3. 提交作业脚本：
```bash
sbatch job.sh
```
4. 查询作业的相关信息
```bash
$ scontrol show job 4749
JobState=COMPLETE
NumNodes=1 NumCPUs=6 NumTasks=6 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
TRES=cpu=6,mem=7728M,node=1,billing=6,gres/gpu=2

$ sacct -j 4749
       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
4749         myFirstGP+    GTX3090   hpc71037          6  COMPLETED      0:0 
4749.batch        batch              hpc71037          6  COMPLETED      0:0 
4749.extern      extern              hpc71037          6  COMPLETED      0:0 
```

5. 查询有什么作业

```bash
$ squeue -u `whoami`
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
              4775   compute interact hpc71037  R      44:15      1 compute01
```

### 例子

#### 看看显卡 nvidia-smi

```bash
#!/bin/bash
#SBATCH -o hhh.txt
#SBATCH -p GTX3090
#SBATCH -N 1             
#SBATCH -n 12
#SBATCH --gres=gpu:2             

nvidia-smi
```

#### 深度学习 conda

```BASH
#!/bin/bash
#SBATCH -J gnt
#SBATCH -o ~/fork/gnt.out
#SBATCH -p A40
#SBATCH -N 1 
#SBATCH -n 6
#SBATCH --gres=gpu:1  

module load anaconda/anaconda3 
source ~/envs/gnt/bin/activate
cd ~/fork/GNT
nvidia-smi
CUDA_VISIBLE_DEVICES=0 python train.py --config configs/gnt_blender.txt --i_img=1 --train_scenes lego --eval_scenes lego
```

#### 多脚本同时运行

```bash
#!/bin/bash
#SBATCH -p A40
#SBATCH -J nerf
#SBATCH -N 1 
#SBATCH -n 6
#SBATCH --gres=gpu:1  
#SBATCH -o nerf.out

module load anaconda/anaconda3 
source ~/envs/nerf/bin/activate
cd ~/fork/nerf-pytorch
nvidia-smi
CUDA_VISIBLE_DEVICES=0 python run_nerf.py --config configs/fern.txt > a.txt &
CUDA_VISIBLE_DEVICES=0 python run_nerf.py --config configs/lego.txt > b.txt &

source ~/envs/dnerf/bin/activate
cd ~/fork/D-NeRF
CUDA_VISIBLE_DEVICES=0 python run_nerf.py --config configs/mutant.txt > c.txt &
CUDA_VISIBLE_DEVICES=0 python run_nerf.py --config configs/lego.txt > d.txt &
wait
```

## glic问题

```bash
>>> from pytorch3d.utils import ico_sphere
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/public/home/hpc2204081200015/EXERCISE/pytorch3d/pytorch3d/utils/__init__.py", line 7, in <module>
    from .camera_conversions import (
  File "/public/home/hpc2204081200015/EXERCISE/pytorch3d/pytorch3d/utils/camera_conversions.py", line 11, in <module>
    from ..renderer import PerspectiveCameras
  File "/public/home/hpc2204081200015/EXERCISE/pytorch3d/pytorch3d/renderer/__init__.py", line 7, in <module>
    from .blending import (
  File "/public/home/hpc2204081200015/EXERCISE/pytorch3d/pytorch3d/renderer/blending.py", line 11, in <module>
    from pytorch3d import _C
ImportError: /lib64/libc.so.6: version `GLIBC_2.32' not found (required by /public/home/hpc2204081200015/EXERCISE/pytorch3d/pytorch3d/_C.cpython-39-x86_64-linux-gnu.so)



(point-avatar) [hpc2204081200015@GPU07 pytorch3d]$ strings /lib64/libc.so.6 | grep "GLIBC_2.3"
GLIBC_2.3
GLIBC_2.3.2
GLIBC_2.3.3
GLIBC_2.3.4
___sys_nerr_GLIBC_2_3
___sys_errlist_GLIBC_2_3
_sys_siglist@@GLIBC_2.3.3
sched_getaffinity@GLIBC_2.3.3
realpath@@GLIBC_2.3
pthread_cond_wait@@GLIBC_2.3.2
regexec@@GLIBC_2.3.4
pthread_cond_timedwait@@GLIBC_2.3.2
__tls_get_addr@@GLIBC_2.3
pthread_cond_broadcast@@GLIBC_2.3.2
_sys_nerr@GLIBC_2.3
pthread_cond_signal@@GLIBC_2.3.2
sys_sigabbrev@@GLIBC_2.3.3
_sys_errlist@GLIBC_2.3
nftw64@@GLIBC_2.3.3
sched_setaffinity@GLIBC_2.3.3
sched_setaffinity@@GLIBC_2.3.4
pthread_cond_init@@GLIBC_2.3.2
pthread_cond_destroy@@GLIBC_2.3.2
nftw@@GLIBC_2.3.3
sched_getaffinity@@GLIBC_2.3.4
```

不支持