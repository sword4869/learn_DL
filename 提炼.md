# 通常我们在计算层数时不考虑输入层

> 线性回归模型


需要注意的是，该图只显示连接模式，即只显示每个输入如何连接到输出，隐去了权重和偏置的值。

![线性回归是一个单层神经网络。](https://cdn.jsdelivr.net/gh/sword4869/pic1@main/images/202407061955888.svg)

所示的神经网络中:

- 输入为$x_1, \ldots, x_d$，因此输入层中的*输入数*（或称为*特征维度*，feature dimensionality）为$d$。

- 网络的输出为$o_1$，因此输出层中的*输出数*是1。


图中神经网络的**层数为1**。我们可以将线性回归模型视为仅由单个人工神经元组成的神经网络，或称为单层神经网络。

> MLP


![picture 1](https://cdn.jsdelivr.net/gh/sword4869/pic1@main/images/202407061955890.png)  

这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。 输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。 

因此，这个多层感知机中的**层数为2**。



```python
nn.Linear(784, 256),
nn.Linear(256, 10))
```
![图 1](https://cdn.jsdelivr.net/gh/sword4869/pic1@main/images/202407061955891.png)  


# 权重是哪个层的

层数为n的神经网络的每一层都有权重, 即除了输入层就开始有了.

这层的权重是上一层的n个神经元到本层m个神经元之间的连线. size=(num_prior, num_current)

这层的偏置是本层m个神经元的. size=(num_current)

> 多层感知机的例子

已知这个多层感知机有d个输入(d个特征)，q个输出，其隐藏层包含h个隐藏单元。



- 隐藏层权重$\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$: 是上一层(输入层, d个神经元)到本层(隐藏层, h个神经元)之间的连线.

  隐藏层偏置$\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$

- 输出层权重$\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$

  输出层偏置$\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。




$\mathbf{X} \in \mathbb{R}^{n \times d}$ 表示输入的样本, 每个样本具有$d$个输入特征。
$\mathbf{H} \in \mathbb{R}^{n \times h}$表示隐藏层的h个神经元输出.($[n \times d][d \times h]=[n \times h]$)
$\mathbf{O} \in \mathbb{R}^{n \times q}$表示输出层的q个神经元的输出.($[n \times h][h \times q]=[n \times q]$)
这里的$n$都表示n个样本的小批量.

$$
\begin{split}\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}\end{split}
$$

其中,在仿射变换之后对每个隐藏单元应用非线性的激活函数.


# 正则化

联系范数, 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。

- $L_2$范数

  一个原因是它对权重向量的大分量施加了巨大的惩罚。这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。在实践中，这可能使它们对单个变量中的观测误差更为稳定。

- $L_1$范数
  
  惩罚会导致模型将权重集中在一小部分特征上，而将其他权重清除为零。这称为*特征选择*（feature selection），这可能是其他场景下需要的