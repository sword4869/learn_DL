- [1. parameter和buffer](#1-parameter和buffer)
  - [1.1. nn.Parameter / nn.parameter.Parameter](#11-nnparameter--nnparameterparameter)
  - [1.2. buffer](#12-buffer)
- [2. 模型参数](#2-模型参数)
  - [2.1. model](#21-model)
  - [2.2. 细分其中某个层](#22-细分其中某个层)
  - [2.3. 参数量](#23-参数量)
- [3. 参数初始化](#3-参数初始化)
  - [3.1. 库](#31-库)
  - [3.2. 自定义](#32-自定义)
  - [3.3. 直接修改](#33-直接修改)
- [4. 参数共享](#4-参数共享)


---

## 1. parameter和buffer

### 1.1. nn.Parameter / nn.parameter.Parameter

```python
class torch.nn.parameter.Parameter(data=None, requires_grad=True)
```

是 `Tensor` 的子类，有一个特殊的性质：将一个不可训练的类型Tensor转换成可以训练的类型parameter, 并将这个parameter绑定到这个module里面（当Parameter作为model的属性与module相关联时，它会被自动添加到Parameters列表中，并且可以使用 `net.Parameters()` 迭代器进行访问）

所以，其比buffer**可学习**，被送到optimizer里优化。


```bash
# graphdeco-inria/gaussian-splatting

# 错的，fused_point_cloud没有梯度，但是 nn.Parameter 会给梯度。
self._xyz = nn.Parameter(fused_point_cloud.requires_grad_(False))
# 对的，真不更新
self._xyz = nn.Parameter(fused_point_cloud, requires_grad=False)


l = [
    {'params': [self._xyz], 'lr': training_args.position_lr_init * self.spatial_lr_scale, "name": "xyz"}
]

self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)
```

PS: nn.Parameter / nn.parameter.Parameter

```python
# nn 包中 __init__.py
# nn/parameter.py  中定义了 class Parameter
from .parameter import (
    Parameter as Parameter,
)
```

例子：

```python
# ViT
    self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, dim))
```

### 1.2. buffer
```python
register_buffer(name, tensor, persistent=True)
```

- `tensor`:

    传入Tensor即可，不需要将其转成gpu。当网络进行`.cuda`时候，会自动将里面的层的参数，buffer等转换成相应的GPU上。

    If `None`, then operations that run on buffers, such as `cuda`, are ignored. 
    
    If `None`, the buffer is not included in the module’s `state_dict`.

- `persistent`:

    网络存储时也会将buffer存下(buffer在`state_dict()`里)，当网络load模型时，会将存储的模型的buffer也进行赋值。
  
    non-persistent buffer is not included in the module’s `state_dict`.


访问：Buffers can be accessed as attributes using given `name`.

buffer不在 `parameters()`里, 但在 `buffers()`里和 `state_dict()`里。所以，buffer 更新不在每次 `optim.step()`（ 会更新 `model.parameters()`）处，而是在 `forward()` 中进行。

```python
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.register_buffer('running_mean', torch.zeros(1))
    
    def forward(self):
        self.running_mean

model = Model() 

>>> dict(model.named_parameters())
{}
>>> dict(model.named_buffers())
{'running_mean': tensor([0.])}
>>> model.state_dict()
OrderedDict([('running_mean', tensor([0.]))])
```

## 2. 模型参数

只要继承了`nn.Module`，都有
- `model.state_dict()`
- `model.parameters()`/`model.named_parameters()`
- 等

什么保存和不保存：
- 只有那些参数可以训练的层才会被保存到模型的权重中, 如卷积层、线性层
- 本身没有参数的层是不保存的；如激活函数层、BN层

内部
- `parameters()` 正是通过 `named_parameters()` 来实现的
- `parameters()` 和 `named_parameters()` 返回的是一个 各层 `nn.Parameter`对象 生成器generator，每个是不仅包含权重的`Tensor`（比如，告诉 device 属性）。
- 而 `state_dict()` 返回的是一个字典, 只包含权重信息。
- 所以保存权重是 `state_dict()`，optimizer 优化的是 `parameters()`

### 2.1. model
```python
from torch import nn
import torch
class MyModel(nn.Module):
    def __init__(self, text_dim=768, flame_dim=156):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(flame_dim, 2048),
            nn.ReLU(),
            nn.Conv2d(1, 1, 3, 1)
        )
        self.conv2 = nn.Conv2d(1, 1, 3, 1)
        pass
    def forward(self, x):
        return self.layers(x)
    
model = MyModel()
>>> model
MyModel(
  (layers): Sequential(
    (0): Linear(in_features=156, out_features=2048, bias=True)
    (1): ReLU()
    (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))
  )
  (conv2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))
)

# 每层的每个参数(`.weight`和`.bias`)都表示为参数类 `nn.Parameter` 的一个实例。
>>> model.parameters()
<generator object Module.parameters at 0x000002996A6BE650>
>>> type(model.parameters())
<class 'generator'>
>>> list(model.parameters())  # 对应 nn.Linear, nn.Conv2d, nn.Conv2d 的 weigth 和 bias
[
    Parameter containing:
    tensor([[-0.0604, -0.0189, -0.0246,  ..., -0.0696, -0.0631, -0.0584],
            [ 0.0375, -0.0487, -0.0492,  ..., -0.0338,  0.0757,  0.0417],
            [ 0.0306, -0.0683,  0.0532,  ...,  0.0034, -0.0688,  0.0165],
            ...,
            [ 0.0713,  0.0111, -0.0294,  ...,  0.0190,  0.0720,  0.0252],
            [-0.0379, -0.0658,  0.0032,  ...,  0.0228, -0.0583, -0.0182],
            [-0.0388,  0.0236,  0.0151,  ...,  0.0198, -0.0018, -0.0701]],
        requires_grad=True), 
    Parameter containing:
    tensor([ 0.0508,  0.0297, -0.0020,  ..., -0.0418,  0.0486, -0.0348],
        requires_grad=True), 
    Parameter containing:
    tensor([[[[ 0.1512,  0.0394,  0.0016],
            [-0.1882,  0.0423,  0.1792],
            [-0.1246,  0.1424,  0.0122]]]], requires_grad=True), 
    Parameter containing:
    tensor([0.0408], requires_grad=True), 
    Parameter containing:
    tensor([[[[ 0.0648,  0.2333,  0.0956],
            [-0.2335,  0.1511,  0.1537],
            [-0.0924, -0.0074,  0.1464]]]], requires_grad=True), 
    Parameter containing:
    tensor([0.1396], requires_grad=True)
]


>>> model.named_parameters()
<generator object Module.named_parameters at 0x000002996A6BE650>
>>> list(model.named_parameters())
[  
    ('layers.0.weight',
    Parameter containing:
    tensor([[-0.0609,  0.0492,  0.0288,  ...,  0.0739, -0.0492,  0.0552],
            [-0.0051,  0.0219,  0.0163,  ...,  0.0451, -0.0603, -0.0385],
            [-0.0207,  0.0668, -0.0400,  ..., -0.0714, -0.0105, -0.0202],
            ...,
            [ 0.0335,  0.0332, -0.0345,  ..., -0.0457, -0.0435,  0.0614],
            [ 0.0499,  0.0519,  0.0354,  ..., -0.0587,  0.0048,  0.0026],
            [-0.0707,  0.0630, -0.0680,  ..., -0.0703, -0.0327, -0.0511]],
            requires_grad=True)),
    ('layers.0.bias',
    Parameter containing:
    tensor([-0.0275,  0.0173, -0.0271,  ...,  0.0204, -0.0572,  0.0049],
            requires_grad=True)),
    ('layers.2.weight',
    Parameter containing:
    tensor([[[[ 0.3022, -0.0507, -0.2963],
                [-0.0192,  0.0941,  0.2055],
                [-0.0626, -0.2946, -0.2451]]]], requires_grad=True)),
    ('layers.2.bias',
    Parameter containing:
    tensor([-0.0253], requires_grad=True)),
    ('conv2.weight',
    Parameter containing:
    tensor([[[[ 0.1499, -0.1262,  0.2431],
                [ 0.1177,  0.0321, -0.0602],
                [-0.2232, -0.1262,  0.2893]]]], requires_grad=True)),
    ('conv2.bias',
    Parameter containing:
    tensor([-0.0060], requires_grad=True))
]




>>> model.state_dict()
OrderedDict(
    [
        ('layers.0.weight',
        tensor([[-0.0609,  0.0492,  0.0288,  ...,  0.0739, -0.0492,  0.0552],
                [-0.0051,  0.0219,  0.0163,  ...,  0.0451, -0.0603, -0.0385],
                [-0.0207,  0.0668, -0.0400,  ..., -0.0714, -0.0105, -0.0202],
                ...,
                [ 0.0335,  0.0332, -0.0345,  ..., -0.0457, -0.0435,  0.0614],
                [ 0.0499,  0.0519,  0.0354,  ..., -0.0587,  0.0048,  0.0026],
                [-0.0707,  0.0630, -0.0680,  ..., -0.0703, -0.0327, -0.0511]])),
        ('layers.0.bias',
        tensor([-0.0275,  0.0173, -0.0271,  ...,  0.0204, -0.0572,  0.0049])),
        ('layers.2.weight',
        tensor([[[[ 0.3022, -0.0507, -0.2963],
                [-0.0192,  0.0941,  0.2055],
                [-0.0626, -0.2946, -0.2451]]]])),
        ('layers.2.bias', tensor([-0.0253])),
        ('conv2.weight',
        tensor([[[[ 0.1499, -0.1262,  0.2431],
                [ 0.1177,  0.0321, -0.0602],
                [-0.2232, -0.1262,  0.2893]]]])),
        ('conv2.bias', tensor([-0.0060]))
    ]
)
>>> type(model.state_dict())
<class 'collections.OrderedDict'>
```
### 2.2. 细分其中某个层

```python
# model.layers 是 model 对象的成员变量
>>> model.layers
Sequential(
  (0): Linear(in_features=156, out_features=2048, bias=True)
  (1): ReLU()
  (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))
)

# model.layers[0] 是 nn.Sequential 的访问方式
>>> model.layers[0]
Linear(in_features=156, out_features=2048, bias=True)
```


```python
>>> model.layers.state_dict()
OrderedDict([('0.weight', tensor([[-0.0604, -0.0189, -0.0246,  ..., -0.0696, -0.0631, -0.0584],
...
>>> list(model.layers.parameters())
[Parameter containing:
tensor([[-0.0604, -0.0189, -0.0246,  ..., -0.0696, -0.0631, -0.0584],
...


>>> model.layers[0].state_dict()
OrderedDict([('weight', tensor([[-0.0604, -0.0189, -0.0246,  ..., -0.0696, -0.0631, -0.0584],
...
>>> list(model.layers[0].parameters())
[Parameter containing:
tensor([[-0.0604, -0.0189, -0.0246,  ..., -0.0696, -0.0631, -0.0584],
...
```

```python
>>> model.layers[0].weight is list(model.parameters())[0]
True


>>> model.layers[0].weight.data
tensor([[-0.0604, -0.0189, -0.0246,  ..., -0.0696, -0.0631, -0.0584],
        [ 0.0375, -0.0487, -0.0492,  ..., -0.0338,  0.0757,  0.0417],
        [ 0.0306, -0.0683,  0.0532,  ...,  0.0034, -0.0688,  0.0165],
        ...,
        [ 0.0713,  0.0111, -0.0294,  ...,  0.0190,  0.0720,  0.0252],
        [-0.0379, -0.0658,  0.0032,  ...,  0.0228, -0.0583, -0.0182],
        [-0.0388,  0.0236,  0.0151,  ...,  0.0198, -0.0018, -0.0701]])
>>> model.layers[0].bias.data
tensor([ 0.0508,  0.0297, -0.0020,  ..., -0.0418,  0.0486, -0.0348])


# model.parameters()
>>> type(list(model.parameters())[0])
<class 'torch.nn.parameter.Parameter'>
>>> list(model.parameters())[0].data            # nn.Linear.weight
tensor([[-0.0604, -0.0189, -0.0246,  ..., -0.0696, -0.0631, -0.0584],
        [ 0.0375, -0.0487, -0.0492,  ..., -0.0338,  0.0757,  0.0417],
        [ 0.0306, -0.0683,  0.0532,  ...,  0.0034, -0.0688,  0.0165],
        ...,
        [ 0.0713,  0.0111, -0.0294,  ...,  0.0190,  0.0720,  0.0252],
        [-0.0379, -0.0658,  0.0032,  ...,  0.0228, -0.0583, -0.0182],
        [-0.0388,  0.0236,  0.0151,  ...,  0.0198, -0.0018, -0.0701]])
>>> list(model.parameters())[1].data            # nn.Linear.bias
tensor([ 0.0508,  0.0297, -0.0020,  ..., -0.0418,  0.0486, -0.0348])


# model.state_dict()
>>> model.state_dict()['layers.0.weight']
tensor([[-0.0604, -0.0189, -0.0246,  ..., -0.0696, -0.0631, -0.0584],
        [ 0.0375, -0.0487, -0.0492,  ..., -0.0338,  0.0757,  0.0417],
        [ 0.0306, -0.0683,  0.0532,  ...,  0.0034, -0.0688,  0.0165],
        ...,
        [ 0.0713,  0.0111, -0.0294,  ...,  0.0190,  0.0720,  0.0252],
        [-0.0379, -0.0658,  0.0032,  ...,  0.0228, -0.0583, -0.0182],
        [-0.0388,  0.0236,  0.0151,  ...,  0.0198, -0.0018, -0.0701]])
>>> model.state_dict()['layers.0.bias']
tensor([ 0.0508,  0.0297, -0.0020,  ..., -0.0418,  0.0486, -0.0348])
```
### 2.3. 参数量

```python
# https://github.com/CompVis/latent-diffusion/blob/a506df5756472e2ebaf9078affdde2c4f1502cd4/ldm/util.py#L71
def count_params(model, verbose=False):
    total_params = sum(p.numel() for p in model.parameters())
    if verbose:
        print(f"{model.__class__.__name__} has {total_params * 1.e-6:.2f} M params.")
    return total_params
```

## 3. 参数初始化

### 3.1. 库
初始化 `.weight` or `.bias`:
```python
# 给定的常数
nn.init.constant_(m.weight, 1)

# 全0初始化
nn.init.zeros_(m.bias)          

# 以均值0和标准差0.01的正态分布
nn.init.normal_(m.weight, mean=0, std=0.01)

# 均居分布
nn.init.uniform_(m.weight, -10, 10)

# xavier
nn.init.xavier_uniform_(m.weight)
```

```python
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

# 应用到网络上
model.apply(init_weights)
```
### 3.2. 自定义
$$
\begin{aligned}
w \sim 
\begin{cases}
U(-10, -5) & \text{ 可能性 } \frac{1}{4} \\
0    & \text{ 可能性 } \frac{1}{2} \\
U(5, 10) & \text{ 可能性 } \frac{1}{4} \\
\end{cases}
\end{aligned}
$$

```python
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5

model.apply(init_weights)
```

### 3.3. 直接修改

```python
model.conv2.weight.data[:] += 1
model.conv2.weight.data[:] = 42
```

## 4. 参数共享

```python
# 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)
net = nn.Sequential(
    nn.Linear(4, 8), 
    nn.ReLU(),
    shared, 
    nn.ReLU(),
    shared, 
    nn.ReLU(),
    nn.Linear(8, 1)
)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
# tensor([True, True, True, True, True, True, True, True])

# 确保它们实际上是同一个对象，而不只是有相同的值
net[2].weight.data[0, 0] = 100
print(net[2].weight.data[0] == net[4].weight.data[0])
# tensor([True, True, True, True, True, True, True, True])
```