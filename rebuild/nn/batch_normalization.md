《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》

内部协变量偏移现象：训练深度神经网络很复杂，因为在训练过程中，随着前一层参数的变化，每层输入的分布也会发生变化。由于需要**较低的学习率和仔细的参数初始化**，这会**减慢训练速度**，并且使得训练具有饱和非线性的模型变得非常困难。

通过标准化层输入来解决该问题。
- 具体方法：将归一化作为模型架构的一部分，并对每个训练小批量执行归一化。
- 好处：批量归一化允许我们使用**更高的学习率**并且**对初始化不那么小心**。
- 它还充当正则化器，在某些情况下消除了 Dropout 的需要。
- 更快：批量归一化应用于最先进的图像分类模型，以减少 14 倍的训练步骤实现相同的精度，并大幅优于原始模型。