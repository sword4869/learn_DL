[关于pytorch中使用detach并不能阻止参数更新这档子事儿](https://zhuanlan.zhihu.com/p/344916574)

根据这篇文章， 控制detach()的网络在 epoch=1时，`DetachLayer1=True`，梯度为0，但权重还更新。

但是现在，pytorch 2.0后，在 epoch=1时，`DetachLayer1=True`，梯度为`None`，权重真不更新了。

## 一般网络


```python
import torch
import numpy as np
import torch.nn as nn
class TestDetach(nn.Module):
    def __init__(self, InDim, HiddenDim, OutDim):
        super().__init__()
        self.layer1 = nn.Linear(InDim, HiddenDim, False)
        self.layer2 = nn.Linear(HiddenDim, OutDim, False)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = x.detach()
        print('forward', x.grad)
        x = self.layer2(x)
        return x
```


```python
def train():
    # 随机生成数据
    N, F = 5000, 196
    x = torch.Tensor(np.random.randn(N, F))
    y = torch.LongTensor(np.random.randint(0, 3, (N,)))
    # 生成损失函数，模型，优化器
    LossFunc = nn.CrossEntropyLoss()
    model = TestDetach(F, 64, 3)
    optimizer = torch.optim.Adam(model.parameters(), lr=1, weight_decay=0.5)
    # 训练
    for epoch in range(3):
        Yhat = model(x)
        Loss = LossFunc(Yhat, y)
        print(f"Epoch {epoch}, Loss: {Loss:.4f}")
        Loss.backward()
        print(model.layer1.weight.data)
        print(model.layer1.weight.grad)
        optimizer.step()
        print(model.layer1.weight.data)
        print(model.layer1.weight.grad)
        optimizer.zero_grad()
        print()
```


```python
train()
```

    forward None
    Epoch 0, Loss: 1.1188
    tensor([[ 0.0696, -0.0170, -0.0488,  ...,  0.0213,  0.0657, -0.0161],
            [ 0.0277, -0.0617, -0.0238,  ...,  0.0202,  0.0674,  0.0204],
            [ 0.0421, -0.0183,  0.0088,  ..., -0.0247,  0.0015,  0.0330],
            ...,
            [ 0.0661, -0.0493, -0.0369,  ..., -0.0413, -0.0202, -0.0501],
            [-0.0235,  0.0060, -0.0438,  ...,  0.0192, -0.0570, -0.0489],
            [ 0.0608,  0.0532,  0.0700,  ...,  0.0183,  0.0471, -0.0702]])
    None
    tensor([[ 0.0696, -0.0170, -0.0488,  ...,  0.0213,  0.0657, -0.0161],
            [ 0.0277, -0.0617, -0.0238,  ...,  0.0202,  0.0674,  0.0204],
            [ 0.0421, -0.0183,  0.0088,  ..., -0.0247,  0.0015,  0.0330],
            ...,
            [ 0.0661, -0.0493, -0.0369,  ..., -0.0413, -0.0202, -0.0501],
            [-0.0235,  0.0060, -0.0438,  ...,  0.0192, -0.0570, -0.0489],
            [ 0.0608,  0.0532,  0.0700,  ...,  0.0183,  0.0471, -0.0702]])
    None
    
    forward None
    Epoch 1, Loss: 3.6285
    tensor([[ 0.0696, -0.0170, -0.0488,  ...,  0.0213,  0.0657, -0.0161],
            [ 0.0277, -0.0617, -0.0238,  ...,  0.0202,  0.0674,  0.0204],
            [ 0.0421, -0.0183,  0.0088,  ..., -0.0247,  0.0015,  0.0330],
            ...,
            [ 0.0661, -0.0493, -0.0369,  ..., -0.0413, -0.0202, -0.0501],
            [-0.0235,  0.0060, -0.0438,  ...,  0.0192, -0.0570, -0.0489],
            [ 0.0608,  0.0532,  0.0700,  ...,  0.0183,  0.0471, -0.0702]])
    None
    tensor([[ 0.0696, -0.0170, -0.0488,  ...,  0.0213,  0.0657, -0.0161],
            [ 0.0277, -0.0617, -0.0238,  ...,  0.0202,  0.0674,  0.0204],
            [ 0.0421, -0.0183,  0.0088,  ..., -0.0247,  0.0015,  0.0330],
            ...,
            [ 0.0661, -0.0493, -0.0369,  ..., -0.0413, -0.0202, -0.0501],
            [-0.0235,  0.0060, -0.0438,  ...,  0.0192, -0.0570, -0.0489],
            [ 0.0608,  0.0532,  0.0700,  ...,  0.0183,  0.0471, -0.0702]])
    None
    
    forward None
    Epoch 2, Loss: 1.4687
    tensor([[ 0.0696, -0.0170, -0.0488,  ...,  0.0213,  0.0657, -0.0161],
            [ 0.0277, -0.0617, -0.0238,  ...,  0.0202,  0.0674,  0.0204],
            [ 0.0421, -0.0183,  0.0088,  ..., -0.0247,  0.0015,  0.0330],
            ...,
            [ 0.0661, -0.0493, -0.0369,  ..., -0.0413, -0.0202, -0.0501],
            [-0.0235,  0.0060, -0.0438,  ...,  0.0192, -0.0570, -0.0489],
            [ 0.0608,  0.0532,  0.0700,  ...,  0.0183,  0.0471, -0.0702]])
    None
    tensor([[ 0.0696, -0.0170, -0.0488,  ...,  0.0213,  0.0657, -0.0161],
            [ 0.0277, -0.0617, -0.0238,  ...,  0.0202,  0.0674,  0.0204],
            [ 0.0421, -0.0183,  0.0088,  ..., -0.0247,  0.0015,  0.0330],
            ...,
            [ 0.0661, -0.0493, -0.0369,  ..., -0.0413, -0.0202, -0.0501],
            [-0.0235,  0.0060, -0.0438,  ...,  0.0192, -0.0570, -0.0489],
            [ 0.0608,  0.0532,  0.0700,  ...,  0.0183,  0.0471, -0.0702]])
    None
    
    

可以看到权重不变， `tensor([-0.0485, -0.0191, -0.0404, -0.0675, -0.0586]`。
权重的梯度一直是None

## 控制detach


```python
import torch
import numpy as np
import torch.nn as nn
class TestDetach(nn.Module):
    def __init__(self, InDim, HiddenDim, OutDim):
        super().__init__()
        self.layer1 = nn.Linear(InDim, HiddenDim, False)
        self.layer2 = nn.Linear(HiddenDim, OutDim, False)

    def forward(self, x, DetachLayer1):  # 多传了一个bool参数，以指示是否detach
        x = torch.relu(self.layer1(x))
        if DetachLayer1:
            x = x.detach()
        print('forward', x.grad)
        x = self.layer2(x)
        return x
```


```python
def train():
    # 随机生成数据
    N, F = 5000, 196
    x = torch.Tensor(np.random.randn(N, F))
    y = torch.LongTensor(np.random.randint(0, 3, (N,)))
    # 生成损失函数，模型，优化器
    LossFunc = nn.CrossEntropyLoss()
    model = TestDetach(F, 64, 3)
    optimizer = torch.optim.Adam(model.parameters(), lr=1, weight_decay=0.5)
    # 训练
    for epoch in range(3):
        # 将DetachLayer1交替置为True和False
        DetachLayer1 = False if epoch % 2 == 0 else True
        Yhat = model(x, DetachLayer1)
        Loss = LossFunc(Yhat, y)
        print(f"Epoch {epoch}, DetachLayer1: {DetachLayer1}, Loss: {Loss:.4f}")
        Loss.backward()
        print(model.layer1.weight.data)
        print(model.layer1.weight.grad)
        optimizer.step()
        print(model.layer1.weight.data)
        print(model.layer1.weight.grad)
        optimizer.zero_grad()
        print()
```


```python
train()
```

    forward None
    Epoch 0, DetachLayer1: False, Loss: 1.1169
    tensor([[ 0.0163, -0.0452,  0.0226,  ..., -0.0509, -0.0273,  0.0067],
            [ 0.0487,  0.0399,  0.0513,  ...,  0.0383,  0.0658, -0.0382],
            [ 0.0127,  0.0086, -0.0123,  ...,  0.0667,  0.0205, -0.0065],
            ...,
            [-0.0514, -0.0572, -0.0212,  ...,  0.0611, -0.0066, -0.0627],
            [ 0.0007, -0.0391,  0.0447,  ...,  0.0360,  0.0402, -0.0023],
            [ 0.0626, -0.0621,  0.0091,  ...,  0.0600, -0.0168, -0.0410]])
    tensor([[ 4.9653e-05,  1.6841e-04,  1.4024e-04,  ..., -3.5658e-04,
             -6.9931e-05, -1.3220e-04],
            [ 7.3195e-04,  1.6677e-04,  4.0484e-05,  ..., -2.7376e-04,
              7.2530e-04,  6.9220e-04],
            [ 4.0941e-04, -3.4236e-04,  1.1325e-04,  ..., -7.4174e-04,
              1.6692e-04, -4.7383e-04],
            ...,
            [ 1.1512e-03, -1.9519e-04,  2.6392e-04,  ..., -4.9245e-04,
              5.1780e-04, -1.6614e-04],
            [ 3.9744e-04,  3.4296e-04,  4.3860e-04,  ...,  3.2766e-04,
              6.3207e-04, -1.6861e-04],
            [ 1.0351e-03,  1.2914e-03,  6.8043e-04,  ...,  2.2072e-04,
              4.8620e-05, -3.5059e-04]])
    tensor([[-0.9837,  0.9548, -0.9774,  ...,  0.9491,  0.9727, -0.9933],
            [-0.9513, -0.9601, -0.9487,  ..., -0.9617, -0.9342,  0.9618],
            [-0.9873, -0.9913,  0.9877,  ..., -0.9333, -0.9795,  0.9935],
            ...,
            [ 0.9486,  0.9428,  0.9788,  ..., -0.9389,  0.9934,  0.9373],
            [-0.9993,  0.9609, -0.9553,  ..., -0.9640, -0.9598,  0.9977],
            [-0.9374,  0.9379, -0.9908,  ..., -0.9400,  0.9832,  0.9590]])
    tensor([[ 4.9653e-05,  1.6841e-04,  1.4024e-04,  ..., -3.5658e-04,
             -6.9931e-05, -1.3220e-04],
            [ 7.3195e-04,  1.6677e-04,  4.0484e-05,  ..., -2.7376e-04,
              7.2530e-04,  6.9220e-04],
            [ 4.0941e-04, -3.4236e-04,  1.1325e-04,  ..., -7.4174e-04,
              1.6692e-04, -4.7383e-04],
            ...,
            [ 1.1512e-03, -1.9519e-04,  2.6392e-04,  ..., -4.9245e-04,
              5.1780e-04, -1.6614e-04],
            [ 3.9744e-04,  3.4296e-04,  4.3860e-04,  ...,  3.2766e-04,
              6.3207e-04, -1.6861e-04],
            [ 1.0351e-03,  1.2914e-03,  6.8043e-04,  ...,  2.2072e-04,
              4.8620e-05, -3.5059e-04]])
    
    forward None
    Epoch 1, DetachLayer1: True, Loss: 68.3408
    tensor([[-0.9837,  0.9548, -0.9774,  ...,  0.9491,  0.9727, -0.9933],
            [-0.9513, -0.9601, -0.9487,  ..., -0.9617, -0.9342,  0.9618],
            [-0.9873, -0.9913,  0.9877,  ..., -0.9333, -0.9795,  0.9935],
            ...,
            [ 0.9486,  0.9428,  0.9788,  ..., -0.9389,  0.9934,  0.9373],
            [-0.9993,  0.9609, -0.9553,  ..., -0.9640, -0.9598,  0.9977],
            [-0.9374,  0.9379, -0.9908,  ..., -0.9400,  0.9832,  0.9590]])
    None
    tensor([[-0.9837,  0.9548, -0.9774,  ...,  0.9491,  0.9727, -0.9933],
            [-0.9513, -0.9601, -0.9487,  ..., -0.9617, -0.9342,  0.9618],
            [-0.9873, -0.9913,  0.9877,  ..., -0.9333, -0.9795,  0.9935],
            ...,
            [ 0.9486,  0.9428,  0.9788,  ..., -0.9389,  0.9934,  0.9373],
            [-0.9993,  0.9609, -0.9553,  ..., -0.9640, -0.9598,  0.9977],
            [-0.9374,  0.9379, -0.9908,  ..., -0.9400,  0.9832,  0.9590]])
    None
    
    forward None
    Epoch 2, DetachLayer1: False, Loss: 183.4053
    tensor([[-0.9837,  0.9548, -0.9774,  ...,  0.9491,  0.9727, -0.9933],
            [-0.9513, -0.9601, -0.9487,  ..., -0.9617, -0.9342,  0.9618],
            [-0.9873, -0.9913,  0.9877,  ..., -0.9333, -0.9795,  0.9935],
            ...,
            [ 0.9486,  0.9428,  0.9788,  ..., -0.9389,  0.9934,  0.9373],
            [-0.9993,  0.9609, -0.9553,  ..., -0.9640, -0.9598,  0.9977],
            [-0.9374,  0.9379, -0.9908,  ..., -0.9400,  0.9832,  0.9590]])
    tensor([[ 0.0089, -0.0077,  0.0037,  ..., -0.0073, -0.0064,  0.0080],
            [ 0.0072, -0.0006, -0.0002,  ...,  0.0021,  0.0074, -0.0035],
            [-0.1297, -0.0065,  0.0015,  ..., -0.0142, -0.0616,  0.0375],
            ...,
            [-0.0442,  0.0352,  0.0133,  ...,  0.0040, -0.0274,  0.0186],
            [-0.0968,  0.0155,  0.0173,  ...,  0.0208, -0.0488,  0.0189],
            [-0.0915,  0.0128, -0.0012,  ...,  0.0020, -0.0554,  0.0196]])
    tensor([[-0.2510,  0.2435, -0.2493,  ...,  0.2430,  0.2480, -0.2536],
            [-0.2440, -0.2446, -0.2419,  ..., -0.2445, -0.2409,  0.2440],
            [-0.2505, -0.2525,  0.2518,  ..., -0.2362, -0.2482,  0.2541],
            ...,
            [ 0.2438,  0.2379,  0.2486,  ..., -0.2395,  0.2532,  0.2379],
            [-0.2560,  0.2432, -0.2452,  ..., -0.2470, -0.2425,  0.2553],
            [-0.2331,  0.2365, -0.2538,  ..., -0.2406,  0.2520,  0.2435]])
    tensor([[ 0.0089, -0.0077,  0.0037,  ..., -0.0073, -0.0064,  0.0080],
            [ 0.0072, -0.0006, -0.0002,  ...,  0.0021,  0.0074, -0.0035],
            [-0.1297, -0.0065,  0.0015,  ..., -0.0142, -0.0616,  0.0375],
            ...,
            [-0.0442,  0.0352,  0.0133,  ...,  0.0040, -0.0274,  0.0186],
            [-0.0968,  0.0155,  0.0173,  ...,  0.0208, -0.0488,  0.0189],
            [-0.0915,  0.0128, -0.0012,  ...,  0.0020, -0.0554,  0.0196]])
    
    

    C:\Users\lab\AppData\Local\Temp\ipykernel_22280\507917766.py:14: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\cb\pytorch_1000000000000\work\build\aten\src\ATen/core/TensorBody.h:491.)
      print('forward', x.grad)
    
