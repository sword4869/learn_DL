{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3740d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torchvision.datasets.FashionMNIST\n",
    "import torchvision\n",
    "# 修改数据集格式\n",
    "from torchvision import transforms\n",
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "# nn块\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8c86a",
   "metadata": {},
   "source": [
    "# net模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ae163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型设置为评估模式\n",
    "net.eval()\n",
    "\n",
    "# 将模型设置为训练模式\n",
    "net.train()\n",
    "\n",
    "# -------------------------------如何使用的例子\n",
    "if isinstance(net, torch.nn.Module):\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0555d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正态分布n(0,1), size (100, 10)\n",
    "torch.normal(0, 1, (100, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be24c66",
   "metadata": {},
   "source": [
    "# 激活函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27651f73",
   "metadata": {},
   "source": [
    "## 修正线性单元（Rectified linear unit，ReLU）\n",
    "\n",
    "$$ \\operatorname{ReLU}(x) = \\max(x, 0) $$\n",
    "\n",
    "求导表现得特别好(要么是0,要么是1)：要么让参数消失，要么让参数通过\n",
    "\n",
    "0点不可导, 不光滑\n",
    "\n",
    "当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。 当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。 相反，ReLU激活函数在正区间的梯度总是1。 因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32e2c5f5",
   "metadata": {},
   "source": [
    "## sigmoid 挤压函数（squashing function）\n",
    "\n",
    "$$ \\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)} $$\n",
    "\n",
    "$$ \\frac{d}{dx} \\operatorname{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\operatorname{sigmoid}(x)\\left(1-\\operatorname{sigmoid}(x)\\right) $$\n",
    "\n",
    "它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值.\n",
    "\n",
    "平滑的、可微的.\n",
    "\n",
    "当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ca91f4e",
   "metadata": {},
   "source": [
    "## tanh函数\n",
    "$$\\operatorname{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}$$\n",
    "\n",
    "将其输入压缩转换到区间(-1, 1)\n",
    "\n",
    "不同的是tanh函数关于坐标系原点中心对称。\n",
    "\n",
    "$$\\frac{d}{dx} \\operatorname{tanh}(x) = 1 - \\operatorname{tanh}^2(x)$$\n",
    "\n",
    "当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc047890",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fa987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "net.apply(init_weights)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e01bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction='none'\n",
    "\n",
    "\n",
    "# 交叉熵损失\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 均方误差\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c27ca199",
   "metadata": {},
   "source": [
    "> 均方误差\n",
    "\n",
    "reduction的意思是维度要不要缩减，以及怎么缩减。\n",
    "\n",
    "$ℓ(x,y)= \\begin{cases} \n",
    "L, where\\ L_i=(x_i−y_i)^2 &\\text{if reduction='None'} \\\\\n",
    "sum(L) &\\text{if reduction='sum'} \\\\\n",
    "mean(L) &\\text{if reduction='mean'} \\\\\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099ed167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  9.],\n",
      "        [25.,  4.]])\n",
      "tensor(42.)\n",
      "tensor(10.5000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[3, 5], [8, 6]], dtype=torch.float32)\n",
    "\n",
    "loss_fn1 = torch.nn.MSELoss(reduction='none')\n",
    "loss1 = loss_fn1(a, b)\n",
    "print(loss1)\n",
    "\n",
    "loss_fn2 = torch.nn.MSELoss(reduction='sum')\n",
    "loss2 = loss_fn2(a, b)\n",
    "print(loss2)\n",
    "\n",
    "loss_fn3 = torch.nn.MSELoss(reduction='mean')\n",
    "loss3 = loss_fn3(a, b)\n",
    "print(loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd4d581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[[3., 0., 0.],\n",
      "         [6., 5., 4.]],\n",
      "\n",
      "        [[0., 3., 7.],\n",
      "         [7., 4., 2.]]])\n",
      "b: tensor([[[6., 3., 7.],\n",
      "         [2., 6., 4.]],\n",
      "\n",
      "        [[0., 4., 2.],\n",
      "         [5., 6., 2.]]])\n",
      "loss_none: tensor([[[ 9.,  9., 49.],\n",
      "         [16.,  1.,  0.]],\n",
      "\n",
      "        [[ 0.,  1., 25.],\n",
      "         [ 4.,  4.,  0.]]])\n",
      "loss_sum: tensor(118.)\n",
      "loss_mean: tensor(9.8333)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0, 9, (2, 2, 3)).float()\n",
    "b = torch.randint(0, 9, (2, 2, 3)).float()\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    " \n",
    "loss_fn1 = torch.nn.MSELoss(reduction='none')\n",
    "loss1 = loss_fn1(a, b)\n",
    "print('loss_none:', loss1)\n",
    " \n",
    "loss_fn2 = torch.nn.MSELoss(reduction='sum')\n",
    "loss2 = loss_fn2(a, b)\n",
    "print('loss_sum:', loss2)\n",
    " \n",
    " \n",
    "loss_fn3 = torch.nn.MSELoss(reduction='mean')\n",
    "loss3 = loss_fn3(a, b)\n",
    "print('loss_mean:', loss3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f58fd",
   "metadata": {},
   "source": [
    "# train_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f71611",
   "metadata": {},
   "source": [
    "![](../image/eval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()  # clear gradients for next train\n",
    "train_loss.backward()  # backpropagation, compute gradients\n",
    "optimizer.step()       # apply gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac1d7d",
   "metadata": {},
   "source": [
    "- 优化器\n",
    "\n",
    "  测试网络时又不用优化器, 那确实没必要写在外面, 也少写一个传参. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_iter, net, loss, optimizer):\n",
    "    # 共有几批\n",
    "    num_batchs = len(train_iter)\n",
    "    # 总平均loss\n",
    "    total_train_loss = 0\n",
    "    for batch, (X, y) in enumerate(train_iter):\n",
    "        # move to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 该批的推断结果\n",
    "        y_hat = net(X)\n",
    "        \n",
    "        train_loss = loss(y_hat, y)\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --------打印进度        \n",
    "        print(f\"\\r[{batch+1:>8d}/{num_batchs:>8d}]  \", end='')\n",
    "\n",
    "    \n",
    "    return total_train_loss / num_batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d79cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------训练\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = train_loop(train_iter, net, loss, optimizer)\n",
    "    print(f'epoch {epoch + 1}, total_train_loss {total_train_loss:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------预测\n",
    "def test_net(test_iter, net, loss):\n",
    "    # 共有几批\n",
    "    num_batchs = len(test_iter)\n",
    "    # 总平均loss, 总平均准确率\n",
    "    total_test_loss, total_correct = 0, 0\n",
    "    # 设定评估模式\n",
    "    net.eval()\n",
    "    # 不要梯度\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(test_iter):\n",
    "            # move to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "\n",
    "            test_loss = loss(y_hat, y)\n",
    "            # 分类0,1,2,3的类别对的上否\n",
    "            correct = (y_hat.argmax(1) == y).float().sum().item()\n",
    "            total_test_loss += test_loss.item()\n",
    "            total_correct += correct/len(X)\n",
    "\n",
    "            # --------打印进度\n",
    "            print(f\"\\r[{batch+1:>8d}/{num_batchs:>8d}]  \", end='')\n",
    "\n",
    "\n",
    "    total_test_loss /= num_batchs\n",
    "    total_correct /= num_batchs\n",
    "    print(\n",
    "        f\"\\nTest: Accuracy: {total_correct:.1%}, Avg loss: {total_test_loss:f}\")\n",
    "    \n",
    "test_net(test_iter, net, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "70c2c951b79418afa68fe3ba4cfcb65558a15271f3a0c4c0e1072a999987ff99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
