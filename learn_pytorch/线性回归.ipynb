{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9382e376",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84512571",
   "metadata": {},
   "source": [
    "## 公式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b71f5",
   "metadata": {},
   "source": [
    "### 线性模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21744a2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5610847d",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01a209",
   "metadata": {},
   "source": [
    "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.$$\n",
    "\n",
    "\n",
    "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
    "\n",
    "$$\\frac{1}{2n}\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2$$\n",
    "\n",
    "\n",
    "$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c861072",
   "metadata": {},
   "source": [
    "### 随机梯度下降\n",
    "\n",
    "$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$\n",
    "\n",
    "我们可以明确地写成如下形式:\n",
    "\n",
    "$$\\begin{aligned} \\mathbf{w} &\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right), \\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}$$\n",
    "\n",
    "- $|\\mathcal{B}|$表示每个小批量中的样本数，这也称为*批量大小*（batch size）。\n",
    "\n",
    "- $\\eta$表示*学习率*（learning rate）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d0bbe",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "## 正态分布与平方损失\n",
    "\n",
    "接下来，我们通过对噪声分布的假设来解读平方损失目标函数。\n",
    "\n",
    "正态分布和线性回归之间的关系很密切。\n",
    "正态分布（normal distribution），也称为*高斯分布*（Gaussian distribution），\n",
    "最早由德国数学家高斯（Gauss）应用于天文学研究。\n",
    "简单的说，若随机变量$x$具有均值$\\mu$和方差$\\sigma^2$（标准差$\\sigma$），其正态分布概率密度函数如下：\n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cd9d2",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是：\n",
    "我们假设了观测中包含噪声，其中噪声服从正态分布。\n",
    "噪声正态分布如下式:\n",
    "\n",
    "$$y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon,$$\n",
    "\n",
    "其中，$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$。\n",
    "\n",
    "因此，我们现在可以写出通过给定的$\\mathbf{x}$观测到特定$y$的*似然*（likelihood）：\n",
    "\n",
    "$$P(y \\mid \\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right).$$\n",
    "\n",
    "现在，根据极大似然估计法，参数$\\mathbf{w}$和$b$的最优值是使整个数据集的*似然*最大的值：\n",
    "\n",
    "$$P(\\mathbf y \\mid \\mathbf X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)}).$$\n",
    "\n",
    "根据极大似然估计法选择的估计量称为*极大似然估计量*。\n",
    "虽然使许多指数函数的乘积最大化看起来很困难，\n",
    "但是我们可以在不改变目标的前提下，通过最大化似然对数来简化。\n",
    "由于历史原因，优化通常是说最小化而不是最大化。\n",
    "我们可以改为*最小化负对数似然*$-\\log P(\\mathbf y \\mid \\mathbf X)$。\n",
    "由此可以得到的数学公式是：\n",
    "\n",
    "$$-\\log P(\\mathbf y \\mid \\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2.$$\n",
    "\n",
    "现在我们只需要假设$\\sigma$是某个固定常数就可以忽略第一项，\n",
    "因为第一项不依赖于$\\mathbf{w}$和$b$。\n",
    "现在第二项除了常数$\\frac{1}{\\sigma^2}$外，其余部分和前面介绍的均方误差是一样的。\n",
    "幸运的是，上面式子的解并不依赖于$\\sigma$。\n",
    "因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
