## 单点测试

$\Delta \mu, \Delta r, \Delta s = F(\gamma(\mu_T), \psi)$

### 直接MLP输出点的最终位置

log_direct

dev_direct



$\mu, \Delta r, \Delta s = F(\gamma(\mu_T), \psi)$

train挺好的，test就是屎

说明纯纯过拟合

![image-20240821084834373](https://cdn.jsdelivr.net/gh/sword4869/pic1@main/images/202408231650082.png)

### nn.Parameter代替位置编码

log_parameter

dev_parameter



直观效果一般，估计得像论文里一样用PSNR指标说话。

## 最终猜想

For most of the works that used the provided datasets, I believe the train/test split is the following.
Subject 1:
train: [MVI_1810, MVI_1814]
test: [MVI_1812]
Subject 2:
train:[MVI_1797, MVI_1801]
test:[MVI_1802]

But as long as you use the same training and testing sequence for all compared methods, I think it shouldn't be a problem. For e.g., in IMavatar, we conduct experiments where we train with [MVI_1810, MVI_1811] and test with MVI_1812, to evaluate quality of out-of-distribution expressions.