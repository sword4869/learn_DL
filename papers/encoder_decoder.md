Encoder-Decoder, 将整个输入序列编码为 **固定长度** 的"中间向量"。但类比"压缩-解压"，会存在"信息过长信息丢失"的问题。

Attention 机制就是为了解决这个问题，将整个输入序列编码成一个向量的序列。

