![图 12](https://cdn.jsdelivr.net/gh/sword4869/pic1@main/images/202407062012494.png)  

one-hot encoding 出来的向量太长了，十万个词汇就对应十万维的向量。

word embedding 的向量短些，而且词意接近的向量展示出来的位置也很接近。